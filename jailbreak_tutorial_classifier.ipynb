{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxjb8q66zhyO"
      },
      "source": [
        "###Welcome!\n",
        "In this notebook, you will find a step-by-step guide to building your first version of a ***jailbreak classifier***, building up its individual parts and seeing how they all contribute to a final deliverable.\n",
        "\n",
        "By the end of this, you will be able to:\n",
        "- Understand what a jailbreak prompt is\n",
        "- Clean and prepare text data\n",
        "- Turn text into numerical features\n",
        "- Train a simple ML model to classify prompts\n",
        "- Evaluate the model\n",
        "- Submit your predictions on test data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yC17uh32hQI"
      },
      "source": [
        "### Step 0: What is a jailbreak classifier?\n",
        "\n",
        "Before we start coding, it's important to understand what we're doing and why we're doing it.\n",
        "\n",
        "With the rise of large language models like ChatGPT, we've come to know of them as very powerful and very useful tools. However, they still have vulnerabilities. The term \"jailbreaking\" refers to users writing prompts intended to trick the AI into:\n",
        "- Giving harmful advice (e.g., \"How to make a bomb\")\n",
        "- Bypassing safety rules (e.g., \"Pretend you are an evil AI\")\n",
        "- Overall producing dangerous or unethical content\n",
        "\n",
        "A **jailbreak classifier** is a system that detects when a user's prompt might be trying to jailbreak the model.\n",
        "\n",
        "**Why this matters:**\n",
        "- Jailbreak classifiers keep AI systems safer and more trustworthy.\n",
        "- They prevent the spread of harmful content.\n",
        "- They're a critical part of AI alignment and safety work.\n",
        "\n",
        "Throughout this practice, we'll build a starter version of a jailbreak classifier. Your system won't be perfect, but it will be a strong first step toward making AI safer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVDoWGoi4tco"
      },
      "source": [
        "### Step 1: Setup\n",
        "First, we just have to make sure that you have the right tools installed.\n",
        "\n",
        "***What you need to do***\n",
        "* Run the code cell below to install the correct libraries\n",
        "\n",
        "*Don't worry if you get some dependency errors here*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVc2gocvzX93"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install scikit-learn datasets --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpNV-kgm6BAh"
      },
      "source": [
        "### Step 2: Data\n",
        "For this classifier, we're going to be using a public dataset downloadable on Hugging Face called `Anthropic/hh-rlhf`. **Before coding**, take a minute to explore the dataset here: https://huggingface.co/datasets/Anthropic/hh-rlhf\n",
        "\n",
        "This dataset was created to train assistant models that are helpful but harmless. Each example contains:\n",
        "- A **prompt** from a user (the \"chosen\" field)\n",
        "- A **response** from the assistant\n",
        "\n",
        "This data was used to fine-tune AI behavior and also contain examples that are meant to test model safety, which is similar to what we want to do. For our purposes, weâ€™ll only use the prompts (from the `chosen` field), and weâ€™ll assign labels to them ourselves based on whether they appear to be trying to jailbreak the model.\n",
        "\n",
        "\n",
        "\n",
        "***What you need to do***\n",
        "* In the code box below, the dataset is loaded into a variable called 'raw_dataset'. You should come up with a simple way to take some of this data and for each prompt, label it with a 0 if it's safe and with a 1 if you think it's a jailbreak attempt.\n",
        "\n",
        "---\n",
        "\n",
        "*Hint 1*: One strategy would be to look for key words in each prompt. What are some words that could indicate that a prompt is an attempt at jailbreaking?\n",
        "\n",
        "---\n",
        "\n",
        "*If you get stuck, there's an example solution that you can find below*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "50FjpKtf7LFC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "raw_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n",
        "\n",
        "# Preview a few rows\n",
        "raw_dataset[0]\n",
        "\n",
        "# You'll extract 2000 prompts from the dataset and label each one using a basic rule:\n",
        "# If the prompt contains any harmful keyword (e.g., \"kill\", \"harm\", \"bomb\", \"attack\"), label it as 1 (jailbreak).\n",
        "# Otherwise, label it as 0 (safe).\n",
        "\n",
        "\n",
        "# 1. Create two empty lists: prompts and labels\n",
        "# prompts = []\n",
        "# labels = []\n",
        "#\n",
        "# 2. Loop through the first 200 examples using raw_dataset.select(range(200))\n",
        "# 3. For each example, extract the 'chosen' prompt\n",
        "# 4. Apply your labeling rule\n",
        "# 5. Append prompt and label to the lists\n",
        "\n",
        "###YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VUNvL34-06u"
      },
      "outputs": [],
      "source": [
        "# Model Solution for Dataset Creation:\n",
        "prompts = []\n",
        "labels  = []\n",
        "KEYWORDS = [\n",
        "    \"kill\", \"harm\", \"bomb\", \"attack\", \"hate\", \"stupid\", \"idiot\", \"dumb\", \"racist\", \"sexist\", \"slur\", \"terror\"\n",
        "]\n",
        "\n",
        "for example in raw_dataset.select(range(2000)):\n",
        "    prompt = example[\"chosen\"]\n",
        "    label  = 1 if any(k in prompt.lower() for k in KEYWORDS) else 0\n",
        "    prompts.append(prompt)\n",
        "    labels.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-c9uTaR_UuN"
      },
      "source": [
        "***Run this provided code after step 2. It will use your data to create a DataFrame, which is a power data structure from the pandas library.***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DUW0jiAf_IuV"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\"prompt\": prompts, \"label\": labels})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKvolrKm7ZXI"
      },
      "source": [
        "### Step 3: Preprocessing the text\n",
        "\n",
        "Now that you have a labelled `DataFrame` called `df`, the next step is to standardise the prompts so the model sees a consistent vocabulary. Small differences such as capital letters can create duplicate tokens and hurt performance.\n",
        "\n",
        "***What you need to do***\n",
        "* Here, weâ€™ll keep preprocessing simple but effective. In the code cell below, complete the 'preprocess()' function with the following functionality:\n",
        "* 1. Lowerâ€‘casing: turn \"BOMB\" and \"bomb\" into the *ame token.\n",
        "* 2. Stripping whitespace: remove accidental leading/trailing spaces.\n",
        "\n",
        "*(In production you might also strip punctuation, normalise Unicode, or apply stemming/lemmatisation, but lowerâ€‘case + strip is enough for this exercise.)*\n",
        "\n",
        "---\n",
        "\n",
        "*If you get stuck, there's an example solution that you can find below*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V47SKCL7hil"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in this function to preprocess the prompts\n",
        "# lowercase everything and remove extra spaces\n",
        "\n",
        "def preprocess(text):\n",
        "  ### YOUR CODE HERE\n",
        "\n",
        "# Apply the preprocess() function to every prompt and store the cleaned text in a new column `clean_prompt`.\n",
        "df['clean_prompt'] = df['prompt'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHo82RosA2VN"
      },
      "outputs": [],
      "source": [
        "# Model Solution for preprocess():\n",
        "def preprocess(text):\n",
        "    return text.lower().strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy6kF5s7BWyW"
      },
      "source": [
        "### Step 4: Convert text into features\n",
        "The most common first step is to represent each prompt as a highâ€‘dimensional vector using **TFâ€‘IDF (Term Frequencyâ€‰Ã—â€‰Inverse Document Frequency)**.\n",
        "\n",
        "**What TFâ€‘IDF does, stepâ€‘byâ€‘step**\n",
        "1. **Build a vocabulary** â€“ the vectoriser scans all cleaned prompts and assigns an index to every unique word (token).  \n",
        "   *If we saw 8â€¯500 unique words, the final feature space will have 8â€¯500 columns.*\n",
        "2. **Term Frequency (TF)** â€“ for a given prompt, count how many times each vocab word appears. More repeats â†’ higher TF.\n",
        "3. **Inverse Document Frequency (IDF)** â€“ words that appear in many prompts (e.g., \"the\") get downâ€‘weighted; niche words get upâ€‘weighted.  \n",
        "   IDF is preâ€‘computed once from the whole corpus.\n",
        "4. **Combine** â€“ each cell in the big matrix is `TF Ã— IDF`.  The result is a sparse matrix (mostly zeros) where each row is one prompt and each column a word weight.\n",
        "\n",
        "**Why we like TFâ€‘IDF here**\n",
        "* Fast, simple, interpretable: you can inspect which words carry the most weight.\n",
        "* Works well for linear models like Logistic Regression.\n",
        "* Requires no deep learning infrastructureâ€”perfect for a first classifier.\n",
        "\n",
        "***What you need to do***\n",
        "* Just run the code cell below.  \n",
        "  `vectorizer.fit_transform()` both fits (learns vocabulary + IDF) and transforms the cleaned prompts into vectors.\n",
        "* The printed `X.shape` tells you `(number_of_prompts, vocabulary_size)` so you can sanityâ€‘check the dimensionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlQOd7_wCJfj"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['clean_prompt'])\n",
        "print(f\"TF-IDF feature matrix shape: {X.shape}\")  # rows = examples, cols = features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ClREGUuCRnj"
      },
      "source": [
        "### Step 5: Convert labels\n",
        "Machineâ€‘learning algorithms require numeric targets.  Our `df` currently stores labels as Python integers (`0` and `1`), but itâ€™s good practice to make the conversion step explicit, and also to sanityâ€‘check class balance before training.\n",
        "\n",
        "***What you need to do***\n",
        "1. **Run the code cell below**â€”it simply extracts the `label` column as a NumPy array `y`.\n",
        "2. Note the printed dictionary, e.g. `{0:Â 4300,Â 1:Â 1700}`.  If the positive class is tiny, youâ€™ll know recall will be challenging and `class_weight` is important.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22Ij4gNsCaOl"
      },
      "outputs": [],
      "source": [
        "# Convert labels to binary array\n",
        "y = df['label'].values\n",
        "print(f\"Labels distribution: {pd.Series(y).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWnA0f49C7dM"
      },
      "source": [
        "### Step 6: Train your classifier\n",
        "Now that X (features) and y (labels) are ready, it is time to teach a model how to tell \"safe\" from \"jailbreak.\" We will use Logistic Regression, a simple algorithm that decides yes or no by adding up weighted word scores.\n",
        "\n",
        "**A brief explanation of Logistic Regression**\n",
        "* Looks at every word in a prompt, multiplies each word by a learned weight, then adds the results.\n",
        "* If the total is high, the prompt is treated as a likely jailbreak (classâ€¯1); if low, it is treated as safe (classâ€¯0).\n",
        "* A knob called C controls how large those weights are allowed to become. Small C = gentle weights (risk of underâ€‘fit); large C = aggressive weights (risk of overâ€‘fit).\n",
        "* For a deeper explanation see scikitâ€‘learnâ€™s short guide: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "\n",
        "What the code below will do\n",
        "1. Split the data into a training portion and a validation portion so we can judge how well the model generalises.\n",
        "2. Train several models with different C values and print each oneâ€™s validation accuracy.\n",
        "3. Select the C that gives the best validation accuracy and retrain one final model on all of the data.\n",
        "\n",
        "***What you need to do***\n",
        "* Run the code cell below and watch the small table that prints C and validation accuracy.\n",
        "* Note which C value has the highest accuracy and use that in the cell box following this next one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6EbfRBRDQA7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train/validation split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Quick hyperâ€‘parameter sweep\n",
        "for C in [0.01, 0.1, 1, 10]:\n",
        "    clf = LogisticRegression(C=C, max_iter=1000, class_weight=\"balanced\")\n",
        "    clf.fit(X_train, y_train)\n",
        "    val_acc = accuracy_score(y_val, clf.predict(X_val))\n",
        "    print(f\"C={C}: validation accuracy = {val_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDS2O2GGDw4F"
      },
      "source": [
        "After tuning, choose a `C` value and train the final model on the full dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4z1j6GiDzCo"
      },
      "outputs": [],
      "source": [
        "# Final model with chosen hyperparameter\n",
        "best_C = 1  # TODO: choose based on your tuning results\n",
        "clf = LogisticRegression(C=best_C, max_iter=1000)\n",
        "clf.fit(X, y)\n",
        "print(\"Final model trained with C=\", best_C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0En0b7W_ECNs"
      },
      "source": [
        "### Step 7: Try out your classifier\n",
        "Your model is now trained and stored in the variable `clf`. The final check before a formal evaluation is to see how it behaves on a few prompts you create yourself. This is a fast way to confirm that the model and vectoriser pipeline work.\n",
        "\n",
        "**What the code below does**\n",
        "1. Defines a short list called `new_prompts` with example questions.  \n",
        "   Feel free to edit or add your own lines.\n",
        "2. Runs each prompt through the same `preprocess` function and TFâ€‘IDF vectoriser used in training, so the model sees data in the expected format.\n",
        "3. Calls `clf.predict` to assign class 0 (safe) or 1 (jailbreak).\n",
        "4. Prints each prompt alongside the modelâ€™s verdict.\n",
        "\n",
        "***What you need to do***\n",
        "* Run the cell asâ€‘is to confirm the pipeline works.  \n",
        "* Replace or append two additional prompts of your own.  \n",
        "* Reâ€‘run the cell and note whether the predictions match your intuition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ABumaFmEKo8"
      },
      "outputs": [],
      "source": [
        "# TODO: Add 3 new example prompts of your own\n",
        "new_prompts = [\n",
        "    \"How can I sabotage someone anonymously?\",\n",
        "    \"How do I bake a chocolate cake?\",\n",
        "    \"Best ways to cheat on a math exam\",\n",
        "    # TODO: Insert new prompts here\n",
        "]\n",
        "\n",
        "# Preprocess & vectorize\n",
        "after_clean = [preprocess(p) for p in new_prompts]\n",
        "X_new       = vectorizer.transform(after_clean)\n",
        "\n",
        "# Predict\n",
        "predictions = clf.predict(X_new)\n",
        "for p, lbl in zip(new_prompts, predictions):\n",
        "    print(f\"Prompt: '{p}'\\n âž¤ Prediction: {'Jailbreak' if lbl == 1 else 'Safe'}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8WT9B0iFxCL"
      },
      "source": [
        "### Step 8: Compare your classifier\n",
        "**Congrats!** Youâ€™ve just built a working jailbreak classifier. ðŸ†\n",
        "\n",
        "The final step is to measure how well it generalises to unseen, realâ€‘world text. We will use the `civil_comments` dataset, which contains online comments scored for toxicity on a 0-1 scale.\n",
        "\n",
        "**What the code below does**\n",
        "1. Installs and loads a 300â€‘row slice of the `civil_comments` test split.\n",
        "2. Converts each comment to the same TFâ€‘IDF space as the training prompts.\n",
        "3. Uses your model `clf` to predict safe vs. harmful.\n",
        "4. Prints four evaluation metrics: **accuracy, precision, recall, and F1.**\n",
        "\n",
        "  *What these numbers mean*\n",
        "  * Accuracy: Out of every comment in the test slice, how many did the classifier label correctly?\n",
        "  * Precision: Of the comments the model flagged as harmful, what fraction were actually harmful?  High precision means few false alarms.\n",
        "  * Recall: Of all the truly harmful comments, what fraction did the model catch?  High recall means few missed toxic posts.\n",
        "  * F1â€‘score: A single number that balances precision and recall (the harmonic mean).  Helpful when the two metrics trade off.\n",
        "\n",
        "***What you need to do***\n",
        "* Run the code cell below.\n",
        "* If your accuracy is at least 0.70, congrats! You've finished this notebook\n",
        "* If results are poor, return to earlier steps and experiment: expand keywords, try a different `C`, or add more training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Mn9LjSwNIQz"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets --quiet\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# â–¶ Load 300 comments with groundâ€‘truth toxicity labels\n",
        "civ = load_dataset(\"civil_comments\", split=\"test[:300]\")\n",
        "X_eval_prompts = [ex[\"text\"] for ex in civ]\n",
        "X_eval_labels  = [1 if ex[\"toxicity\"] >= 0.5 else 0 for ex in civ]  # 1 = harmful / toxic, 0 = safe\n",
        "\n",
        "# Vectorize & predict\n",
        "X_eval_vec = vectorizer.transform([preprocess(p) for p in X_eval_prompts])\n",
        "your_preds = clf.predict(X_eval_vec)\n",
        "\n",
        "# Metrics\n",
        "a = accuracy_score(X_eval_labels, your_preds)\n",
        "p = precision_score(X_eval_labels, your_preds)\n",
        "r = recall_score(X_eval_labels, your_preds)\n",
        "f = f1_score(X_eval_labels, your_preds)\n",
        "\n",
        "print(\"-- Your model on Civil Comments hidden set --\")\n",
        "print(f\"\"\"Accuracy : {a :.3f}\n",
        "Precision: {p:.3f}\n",
        "Recall   : {r :.3f}\n",
        "F1â€‘score : {f  :.3f}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GMTEgYXMLjr"
      },
      "source": [
        "#### Conclusion\n",
        "You now have a fullyâ€‘working jailbreak classifier pipeline:\n",
        "1. **Gathered data** from a public alignment dataset and created your own labels.\n",
        "2. **Cleaned the text** and translated words into TFâ€‘IDF vectors.\n",
        "3. **Trained, tuned, and validated** a Logistic Regression model with balanced class weights.\n",
        "4. **Spotâ€‘checked** predictions on fresh prompts.\n",
        "5. **Measured realâ€‘world performance** on a toxicity corpus and interpreted four key metrics.\n",
        "\n",
        "If you want to keep going with this project, here some ideas to get you started:\n",
        "* **Implement more features** â€“ experiment with bigrams, character nâ€‘grams, or sentenceâ€‘transformer embeddings.\n",
        "* **Test alternate models** â€“ test linear SVM, gradient boosting, or a small neural network.\n",
        "* **Deployment** â€“ wrap the pipeline in a simple API (Flask/FastAPI) and use it to screen live prompts.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
